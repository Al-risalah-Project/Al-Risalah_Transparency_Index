{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff6667-851a-4f26-b6a7-24cb1cf61f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CAMeL-Lab/bert-base-arabic-camelbert-mix-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"CAMeL-Lab/bert-base-arabic-camelbert-mix-ner\")\n",
    "\n",
    "# Load a pre-trained NER pipeline for Arabic\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Sample Arabic text\n",
    "text = \"رام الله هي مدينة فلسطينية تقع في الضفة الغربية.\"\n",
    "\n",
    "# Run the NER model\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "# Inspect the output structure\n",
    "if len(ner_results) > 0:\n",
    "    first_result = ner_results[0]\n",
    "    print(\"Keys in the returned entity dict:\", first_result.keys())\n",
    "\n",
    "# Print the recognized named entities\n",
    "for entity in ner_results:\n",
    "    entity_word = entity.get('word', 'N/A')\n",
    "    entity_label = entity.get('entity', '/A').upper()  # or use your logic if it differs\n",
    "    entity_score = entity.get('score', 'N/A')\n",
    "    print(f\"Entity: {entity_word}, Label: {entity_label}, Score: {entity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec429666-56b6-4825-96bc-8a97d45c4e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Download stopwords if needed\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('articles.db')\n",
    "\n",
    "# Extract the articles data into a Pandas DataFrame\n",
    "df = pd.read_sql_query(\"SELECT headline, published, category, content FROM articles\", conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "\n",
    "# Function to convert and parse Arabic dates\n",
    "def parse_arabic_date(date_str):\n",
    "    date_pattern = re.compile(r'(\\w+)\\s+(\\d{4})\\s\\.\\sالساعة:\\s(\\d{2}:\\d{2}\\s[صم])')\n",
    "    match = date_pattern.search(date_str)\n",
    "\n",
    "    if match:\n",
    "        arabic_month = match.group(1)\n",
    "        year = match.group(2)\n",
    "        time = match.group(3).replace('م', 'PM').replace('ص', 'AM')\n",
    "\n",
    "        english_month = arabic_to_english_months.get(arabic_month)\n",
    "\n",
    "        english_date_str = f'{english_month} {year} {time}'\n",
    "\n",
    "        try:\n",
    "            return pd.to_datetime(\n",
    "                english_date_str,\n",
    "                format='%B %Y %I:%M %p',\n",
    "                errors='coerce'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing date: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply date parsing\n",
    "df['published'] = df['published'].apply(parse_arabic_date)\n",
    "\n",
    "# Drop rows with parsing errors in the 'published' field\n",
    "df.dropna(subset=['published'], inplace=True)\n",
    "\n",
    "# Clean and normalize text content\n",
    "def clean_arabic_text(text):\n",
    "    # Normalize Arabic diacritics and punctuation\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove Arabic stop words\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "# Setup NER model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CAMeL-Lab/bert-base-arabic-camelbert-mix-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"CAMeL-Lab/bert-base-arabic-camelbert-mix-ner\")\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "def ner_preserve_entities(text, max_length=500):\n",
    "    # Tokenize text and split into chunks\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [' '.join(tokens[i:i + max_length]) for i in range(0, len(tokens), max_length)]\n",
    "\n",
    "    preserved_entities = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk.split())\n",
    "        ner_results = ner_pipeline(chunk_text)\n",
    "\n",
    "        last_pos = 0\n",
    "        for entity in ner_results:\n",
    "            entity_start, entity_end, entity_word = entity['start'], entity['end'], entity['word']\n",
    "            preserved_entities.append(chunk_text[last_pos:entity_start].strip())  # Add non-entity text\n",
    "            preserved_entities.append(entity_word.replace(' ', '_'))  # Preserve entity with underscore\n",
    "            last_pos = entity_end\n",
    "\n",
    "        preserved_entities.append(chunk_text[last_pos:].strip())  # Add remaining text\n",
    "\n",
    "    return ' '.join(filter(None, preserved_entities)).strip()\n",
    "\n",
    "# Process text to clean, remove stop words, and apply NER\n",
    "def process_text(text):\n",
    "    text = clean_arabic_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = ner_preserve_entities(text)\n",
    "    return text\n",
    "\n",
    "# Apply text processing\n",
    "df['content'] = df['content'].apply(process_text)\n",
    "\n",
    "# Tokenize and prepare for topic modeling\n",
    "texts = [content.split() for content in df['content']]\n",
    "\n",
    "# Create dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# The resulting 'corpus' and 'dictionary' are ready for LDA modeling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
